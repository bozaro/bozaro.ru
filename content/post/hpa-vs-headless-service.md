---
date: 2021-02-12
draft: true
title: HPA vs Headless Service
slug: hpa-vs-headless-service
tags:
- k8s
- scaling
categories:
- Kubernetes
menu: main
---

Мы столкнулись с достаточно занятным поведением при работе с Headless-сервисом в Kubernetes. В нашем случае проблема
возникла с mongos, но она актуальная для любого Headless-сервиса.

На одном из проектов мы используем MongoDB и Kubernetes. У MongoDB есть компонент: mongos. Через него выполняются
запросы в шардированном MongoDB кластере. До переезда в Kubernetes сервисы mongos устанавливались непосредственно
на каждый хост.

При переезде сервисов в Kubernetes с mongos начались проблемы и мы отселили их в пул headless-сервиса, которые
автоматически масштабируются через HPA (Horizontal Pod Autoscaler).

Через некоторое время выяснилось, что приложению при уменьшении количества POD-ов с mongos становится не очень хорошо.
<!--more-->
Путем отладки выяснилось, что приложение подвисает именно на при попытке установить подключение с mongos-ом (net.Dial
в терминах Go) и по времени совпадает с остановкой какого-либо POD-а.

Для начала надо уточнить, что такое Headless-сервис: это сервис, который не использует отдельный ClusterIP для
маршрутизации запросов, а просто под определённым DNS именем публикует список IP всех POD-ов, которые в этот
сервис входят.

Headless-сервисы полезны, когда приложение само должно управлять тем, к какому POD-у подключаться, например:

 - mongodb клиент использует IP сервера, с которым он работает для того, чтобы запросы для одного курсора
   шли на один хост (курсор "живёт" на mongos). В случае использования ClusterIP могут "теряться" курсоры
   даже для коротких запросов;
 - gRPC клиенты держат по одному соединению с сервисами и сами управляют запросами, мультиплексируюя запросы
   к одному серверу. В случае использования CLusterIP клиент может создать одно подключение и нагружать ровно
   один POD сервера.

Так как клиент сам управляет, к каким POD-ам он подключается, возможна ситуация, когда клиент помнит IP адрес
уже удалённого POD-а. Причины этого просты:

 - список POD-ов передаётся через DNS, а DNS кэшируется;
 - клиент сам по себе кэширует ответы от DNS и список сервисов.

Что же происходит в случае, если клиент пытается подключиться к уже несуществующему POD-у?

А в этом случае запросы уходят уже на немаршрутизируемый хост и на них никто не отвечает. Так как ответа нет,
то клиент начинает слать повторные запросы на подключение пока не пробьёт таймаут.

При этом, в случае если POD еще не поднялся или был отстрелен по Out of Memory, но еще не был удалён, то при
попытке подключиться клиент получает ошибку "connection refused" практически сразу. И это гораздо более гуманное
решение, чем ждать у моря погоды пока не пробьём таймаут.

Когда стала понятна причина, решить проблему было делом техники:

 - Мы добавили ожидание сигнала SIGTERM в POD-е с mongos-ом. При получении этого сигнала мы продолжали работать еще 45
   секунд до времени инвалидации DNS (чтобы адреса новых Pod-ов доехали до клиента). После этой паузы завершали mongos
   и делали еще одну паузу в 15 секунд (чтобы переподключение по старому IP отшивалось по ошибке connection refused, а
   не таймауту).
 - Мы выставили `terminationGracePeriodSeconds` в две минуты, чтобы POD принудительно не отстрелили до его завершения.

## Как поиграться с этой проблемой локально?
Эту ситуацию можно легко воспроизвести в MiniKube на примере nginx.

Для этого надо понадобится headless Service:
{{< code file="hpa-vs-headless-service/service.yml" language="yaml" label="service.yml" >}}

И тестовая утилита:
{{< code file="hpa-vs-headless-service/dialer.go" language="go" label="dialer.go" >}}

Запустим тестовую утилиту для подключения к сервису nginx по 80-ому порту. Она будет выводить результат
попытки подключиться к сервису (пока не успешный, так как сервис смотрит в никуда):
{{< code file="hpa-vs-headless-service/example.sh" language="sh" label="example.sh" >}}

Вывести она должна что-то вида:
```text
16:57:19.986: === nginx:80
16:57:19.988: +++ dial tcp: lookup nginx on 10.96.0.10:53: server misbehaving
```
Пока оставим окно с утилитой и потом будем в него посматривать.

### Простой Deployment без задержек
Добавим в сервис Deployment:
{{< code file="hpa-vs-headless-service/nginx.yml" language="yaml" label="nginx.yml" >}}

На боевом Deployment-е должны быть так же livenessProbe и readinessProbe. Но в данном эксперименте
они будут только мешать.

И сделаем "обновление" Deployment-а:
{{< code file="hpa-vs-headless-service/update.sh" language="sh" label="update.sh" >}}

При изменении аннотации Pod-а произойдёт перевыкатка Deployment-а. При этом важно отметить, что схема
выкатки по-умолчанию: поднять новый Pod и только затем погасить старый Pod-ы. То есть всегда будет
запущен как минимум один Pod.

В выводе тестовой утилиты мы увидим примерно следующее (комментарии добавлены отдельно):
```ini
# Здесь мы подключились к созданному Deployment-у и до обновления попытки
# подключения были успешны
17:04:08.288: +++ connected (172.17.0.10:80)
17:07:32.187: --- connected (172.17.0.10:80): 3m23.899438044s
# Здесь завершился nginx при остановке Pod-а, но клиент еще идет по старому
# кэшированному IP.
# Так как Pod существует, мы быстро получаем ошибку "connection refused"
17:07:32.187: +++ dial tcp 172.17.0.10:80: connect: connection refused
17:07:32.488: --- dial tcp 172.17.0.10:80: connect: connection refused: 301.155902ms
# Старый Pod уже удалён, но клиент всё еще идет по старому кэшированному IP.
# Так как по IP адресу уже никто не отвечает, мы пробиваем таймаут.
17:07:32.488: +++ dial tcp 172.17.0.10:80: i/o timeout
17:07:38.448: --- dial tcp 172.17.0.10:80: i/o timeout: 5.960150161s
# Старый IP покинул кэш и мы подключились к новому Pod-у.
17:07:38.448: +++ connected (172.17.0.7:80)
```

### Добавляем задержку перед удалением Pod-а.
Добавим в Deployment паузу после завершения сервиса, чтобы вместо долгого таймаута получать
быстрый "connection refused":
{{< code file="hpa-vs-headless-service/nginx-add-shutdown.sh" language="sh" label="nginx-add-shutdown.sh" >}}

И еще раз сделаем "обновление" Deployment-а:
{{< code file="hpa-vs-headless-service/update.sh" language="sh" label="update.sh" >}}

В выводе тестовой утилиты мы увидим примерно следующее (комментарии добавлены отдельно):
```ini
# Здесь мы подключились к созданному Deployment-у и до обновления попытки
# подключения были успешны
17:58:10.389: +++ connected (172.17.0.7:80)
18:00:53.687: --- connected (172.17.0.7:80): 2m43.29763747s
# Здесь завершился nginx при остановке Pod-а, но клиент еще идет по старому
# кэшированному IP.
# Так как Pod существует, мы быстро получаем ошибку "connection refused".
# Существовать Pod будет до тех пор пока не завершится sleep после nginx.
18:00:53.687: +++ dial tcp 172.17.0.7:80: connect: connection refused
18:01:10.491: --- dial tcp 172.17.0.7:80: connect: connection refused: 16.804114254s
# Старый IP покинул кэш и мы подключились к новому Pod-у.
18:01:10.491: +++ connected (172.17.0.10:80)
```

### Добавляем задержку перед остановкой Pod-а.
Добавим в Deployment паузу перед завершением сервиса, чтобы сервис отвечал, пока адрес Pod-а не покинет
кэш на клиенте:
{{< code file="hpa-vs-headless-service/nginx-add-gracefull.sh" language="sh" label="nginx-add-gracefull.sh" >}}

И еще раз сделаем "обновление" Deployment-а:
{{< code file="hpa-vs-headless-service/update.sh" language="sh" label="update.sh" >}}

В выводе тестовой утилиты мы увидим примерно следующее (комментарии добавлены отдельно):
```ini
# Здесь мы подключились к созданному Deployment-у и до обновления попытки
# подключения были успешны
18:05:10.589: +++ connected (172.17.0.7:80)
18:07:10.689: --- connected (172.17.0.7:80): 2m0.099149168s
# Старый IP покинул кэш и мы подключились к новому Pod-у.
# Старый Pod еще отвечает и из-за этого переключение прошло гладко.
18:07:10.689: +++ connected (172.17.0.10:80)
```

### Какие нужны задержки?
Итого, для гладкого переключения необходимо две задержки:

 - Между SIGTERM и остановкой приложения.
   
   Эта задержка должна быть не меньше, чем временя жизни записи в DNS кэше.
   
   Делать эту паузу больше, чем сумма времени жизни записи в DNS кэше и времени жизни записи в кэше приложения не имеет
   особого смысла.
   
 - Между остановкой приложения и завершением Pod-а.
   
   Эта задержка должна быть подобрана так, чтобы с момента получения SIGTERM и до завершения Pod-а прошло время не
   меньше суммы времени жизни записи в DNS кэше и времени жизни записи в кэше приложения.
